<!DOCTYPE html>
<html lang="en">
    <!-- Head -->
    <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Metadata, OpenGraph and Schema.org -->

    <!-- Website verification -->
    <meta name="google-site-verification" content="a7Hg3t42hRO4FzsshIDs_GatXm4jXof2OKEJl4Oy7e8">

<!-- Standard metadata -->
<meta charset="utf-8">
<!-- <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> -->
<meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>Publications | Kalin  Stefanov</title>
<meta name="author" content="Kalin  Stefanov">
<meta name="description" content="">

<!-- OpenGraph -->

<!-- Schema.org -->


<!-- Bootstrap & MDB -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

<!-- Styles -->

    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%99%BE%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">


<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="http://localhost:4000/publications/">

<!-- Dark Mode -->


<!-- Three.js -->
<script async src="https://unpkg.com/es-module-shims@1.6.3/dist/es-module-shims.js"></script>

<script type="importmap">
  {
    "imports": {
      "three": "https://unpkg.com/three@v0.152.2/build/three.module.js",
      "three/addons/": "https://unpkg.com/three@v0.152.2/examples/jsm/"
    }
  }
</script>

    </head>
    <!-- Body -->
    <body class="fixed-top-nav ">
        <!-- Header --><header>
    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
            <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Kalin </span>Stefanov</a>
            
            <!-- Navbar Toggle -->
            <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar top-bar"></span>
                <span class="icon-bar middle-bar"></span>
                <span class="icon-bar bottom-bar"></span>
            </button>
            
            <div class="collapse navbar-collapse text-right" id="navbarNav">
                <ul class="navbar-nav ml-auto flex-nowrap">
                    
                    <!-- About -->
                    <li class="nav-item ">
                        <a class="nav-link" href="/">Home</a>
                    </li>
                    
                    <!-- Blog -->
                    
                    
                    <!-- Other pages -->
                    <li class="nav-item ">
                        <a class="nav-link" href="/projects/">Projects</a>
                    </li>
                    <li class="nav-item active">
                        <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a>
                    </li>
<!-- Toogle theme mode -->
                </ul>
            </div>
        </div>
    </nav>
    <!-- Scrolling Progress Bar -->
    
        <progress id="progress" value="0">
            <div class="progress-container">
                <span class="progress-bar"></span>
            </div>
        </progress>
</header>

        <!-- Content -->
        <div class="container mt-5">
            <div class="post">
    <header class="post-header">
        <h1 class="post-title"><span class="font-weight-bold">Publications</span></h1>
        <p class="desc">List of research publications</p>
    </header>

    <article>
        <div class="publications">
  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="2211.06627" class="col-sm-8">
        <!-- Title -->
        <div class="title">MARLIN: Masked Autoencoder for Facial Video Representation Learning</div>
        <!-- Author -->
        <div class="author">
            

            Zhixi Cai, Shreya Ghosh, <strong>Kalin Stefanov</strong>, Abhinav Dhall, Jianfei Cai, Hamid Rezatofighi, Reza Haffari, and Munawar Hayat</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            2023
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/2211.06627.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
                <a href="https://github.com/ControlNet/MARLIN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>This paper proposes a self-supervised approach to learn universal facial representations from videos, that can transfer across a variety of facial analysis tasks such as Facial Attribute Recognition (FAR), Facial Expression Recognition (FER), DeepFake Detection (DFD), and Lip Synchronization (LS). Our proposed framework, named MARLIN, is a facial video masked autoencoder, that learns highly robust and generic facial embeddings from abundantly available non-annotated web crawled facial videos. As a challenging auxiliary task, MARLIN reconstructs the spatio-temporal details of the face from the densely masked facial regions which mainly include eyes, nose, mouth, lips, and skin to capture local and global aspects that in turn help in encoding generic and transferable features. Through a variety of experiments on diverse downstream tasks, we demonstrate MARLIN to be an excellent facial video encoder as well as feature extractor, that performs consistently well across a variety of downstream tasks including FAR (1.13% gain over supervised benchmark), FER (2.64% gain over unsupervised benchmark), DFD (1.86% gain over unsupervised benchmark), LS (29.36% gain for Frechet Inception Distance), and even in low data regime. Our code and models are available at https://github.com/ControlNet/MARLIN.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">2211.06627</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cai, Zhixi and Ghosh, Shreya and Stefanov, Kalin and Dhall, Abhinav and Cai, Jianfei and Rezatofighi, Hamid and Haffari, Reza and Hayat, Munawar}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MARLIN: Masked Autoencoder for Facial Video Representation Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2211.06627}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{2211.06627.pdf}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/ControlNet/MARLIN}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="2305.01979" class="col-sm-8">
        <!-- Title -->
        <div class="title">“Glitch in the Matrix!”: A Large Scale Benchmark for Content Driven Audio-Visual Forgery Detection and Localization</div>
        <!-- Author -->
        <div class="author">
            

            Zhixi Cai, Shreya Ghosh, Abhinav Dhall, Tom Gedeon, <strong>Kalin Stefanov</strong>, and Munawar Hayat</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            2023
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/2305.01979.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
                <a href="https://github.com/ControlNet/LAV-DF" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>Most deepfake detection methods focus on detecting spatial and/or spatio-temporal changes in facial attributes. This is because available benchmark datasets contain mostly visual-only modifications. However, a sophisticated deepfake may include small segments of audio or audio-visual manipulations that can completely change the meaning of the content. To addresses this gap, we propose and benchmark a new dataset, Localized Audio Visual DeepFake (LAV-DF), consisting of strategic content-driven audio, visual and audio-visual manipulations. The proposed baseline method, Boundary Aware Temporal Forgery Detection (BA-TFD), is a 3D Convolutional Neural Network-based architecture which efficiently captures multimodal manipulations. We further improve (i.e. BA-TFD+) the baseline method by replacing the backbone with a Multiscale Vision Transformer and guide the training process with contrastive, frame classification, boundary matching and multimodal boundary matching loss functions. The quantitative analysis demonstrates the superiority of BA- TFD+ on temporal forgery localization and deepfake detection tasks using several benchmark datasets including our newly proposed dataset. The dataset, models and code are available at https://github.com/ControlNet/LAV-DF.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">2305.01979</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{``Glitch in the Matrix!'': A Large Scale Benchmark for Content Driven Audio-Visual Forgery Detection and Localization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cai, Zhixi and Ghosh, Shreya and Dhall, Abhinav and Gedeon, Tom and Stefanov, Kalin and Hayat, Munawar}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2305.01979}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{2305.01979.pdf}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/ControlNet/LAV-DF}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1145/3503161.3551605" class="col-sm-8">
        <!-- Title -->
        <div class="title">Graph-Based Group Modelling for Backchannel Detection</div>
        <!-- Author -->
        <div class="author">
            

            Garima Sharma, <strong>Kalin Stefanov</strong>, Abhinav Dhall, and Jianfei Cai</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the ACM International Conference on Multimedia</em>, 2022
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/3503161.3551605.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>The brief responses given by listeners in group conversations are known as backchannels rendering the task of backchannel detection an essential facet of group interaction analysis. Most of the current backchannel detection studies explore various audio-visual cues for individuals. However, analysing all group members is of utmost importance for backchannel detection, like any group interaction. This study uses a graph neural network to model group interaction through all members’ implicit and explicit behaviours. The proposed method achieves the best and second best performance on agreement estimation and backchannel detection tasks, respectively, of the 2022 MultiMediate: Multi-modal Group Behaviour Analysis for Artificial Mediation challenge.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3503161.3551605</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sharma, Garima and Stefanov, Kalin and Dhall, Abhinav and Cai, Jianfei}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Graph-Based Group Modelling for Backchannel Detection}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450392037}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3503161.3551605}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3503161.3551605}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ACM International Conference on Multimedia}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7190--7194}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{backchannel detection, agreement estimation, graph neural networks}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Lisboa, Portugal}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{MM '22}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{3503161.3551605.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10034605" class="col-sm-8">
        <!-- Title -->
        <div class="title">Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization</div>
        <!-- Author -->
        <div class="author">
            

            Zhixi Cai, <strong>Kalin Stefanov</strong>, Abhinav Dhall, and Munawar Hayat</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the International Conference on Digital Image Computing: Techniques and Applications</em>, 2022
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/10034605.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
                <a href="https://github.com/ControlNet/LAV-DF" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>Due to its high societal impact, deepfake detection is getting active attention in the computer vision community. Most deepfake detection methods rely on identity, facial attributes, and adversarial perturbation-based spatio-temporal modifications at the whole video or random locations while keeping the meaning of the content intact. However, a sophisticated deepfake may contain only a small segment of video/audio manipulation, through which the meaning of the content can be, for example, completely inverted from a sentiment perspective. We introduce a content-driven audio-visual deepfake dataset, termed Localized Audio Visual DeepFake (LAV-DF), explicitly designed for the task of learning temporal forgery localization. Specifically, the content-driven audio-visual manipulations are performed strategically to change the sentiment polarity of the whole video. Our baseline method for benchmarking the proposed dataset is a 3DCNN model, termed as Boundary Aware Temporal Forgery Detection (BA-TFD), which is guided via contrastive, boundary matching, and frame classification loss functions. Our extensive quantitative and qualitative analysis demonstrates the proposed method’s strong performance for temporal forgery localization and deepfake detection tasks.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10034605</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cai, Zhixi and Stefanov, Kalin and Dhall, Abhinav and Hayat, Munawar}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Digital Image Computing: Techniques and Applications}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-10}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/DICTA56598.2022.10034605}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{10034605.pdf}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/ControlNet/LAV-DF}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="0636" class="col-sm-8">
        <!-- Title -->
        <div class="title">Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder for Image Reconstruction and Generation</div>
        <!-- Author -->
        <div class="author">
            

            Mohammad Adiban, <strong>Kalin Stefanov</strong>, Sabato M. Siniscalchi, and Giampiero Salvi</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the British Machine Vision Conference</em>, 2022
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/0636.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
                <a href="https://github.com/mohammad-adiban/Video-Prediction" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>We propose a multi-layer variational autoencoder method, we call HR-VQVAE, that learns hierarchical discrete representations of the data. By utilizing a novel objective function, each layer in HR-VQVAE learns a discrete representation of the residual from previous layers through a vector quantized encoder. Furthermore, the representations at each layer are hierarchically linked to those at previous layers. We evaluate our method on the tasks of image reconstruction and generation. Experimental results demonstrate that the discrete representations learned by HR-VQVAE enable the decoder to reconstruct high-quality images with less distortion than the baseline methods, namely VQVAE and VQVAE-2. HR-VQVAE can also generate high-quality and diverse images that outperform state-of-the-art generative models, providing further verification of the efficiency of the learned representations. The hierarchical nature of HR-VQVAE i) reduces the decoding search time, making the method particularly suitable for high-load tasks and ii) allows to increase the codebook size without incurring the codebook collapse problem.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">0636</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Adiban, Mohammad and Stefanov, Kalin and Siniscalchi, Sabato M. and Salvi, Giampiero}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hierarchical Residual Learning Based Vector Quantized Variational Autoencoder for Image Reconstruction and Generation}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the British Machine Vision Conference}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{BMVA} Press}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://bmvc2022.mpi-inf.mpg.de/0636.pdf}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{0636.pdf}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/mohammad-adiban/Video-Prediction}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="2207.08380" class="col-sm-8">
        <!-- Title -->
        <div class="title">Visual Representations of Physiological Signals for Fake Video Detection</div>
        <!-- Author -->
        <div class="author">
            

            <strong>Kalin Stefanov</strong>, Bhawna Paliwal, and Abhinav Dhall</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            2022
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/2207.08380.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>Realistic fake videos are a potential tool for spreading harmful misinformation given our increasing online presence and information intake. This paper presents a multimodal learning-based method for detection of real and fake videos. The method combines information from three modalities - audio, video, and physiology. We investigate two strategies for combining the video and physiology modalities, either by augmenting the video with information from the physiology or by novelly learning the fusion of those two modalities with a proposed Graph Convolutional Network architecture. Both strategies for combining the two modalities rely on a novel method for generation of visual representations of physiological signals. The detection of real and fake videos is then based on the dissimilarity between the audio and modified video modalities. The proposed method is evaluated on two benchmark datasets and the results show significant increase in detection performance compared to previous methods.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">2207.08380</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visual Representations of Physiological Signals for Fake Video Detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stefanov, Kalin and Paliwal, Bhawna and Dhall, Abhinav}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2207.08380}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{2207.08380.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1109/ICPR48806.2021.9413345" class="col-sm-8">
        <!-- Title -->
        <div class="title">Spatial Bias in Vision-Based Voice Activity Detection</div>
        <!-- Author -->
        <div class="author">
            

            <strong>Kalin Stefanov</strong>, Mohammad Adiban, and Giampiero Salvi</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the International Conference on Pattern Recognition</em>, 2021
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/ICPR48806.2021.9413345.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>We develop and evaluate models for automatic vision-based voice activity detection (VAD) in multiparty human-human interactions that are aimed at complementing acoustic VAD methods. We provide evidence that this type of vision-based VAD models are susceptible to spatial bias in the dataset used for their development; the physical settings of the interaction, usually constant throughout data acquisition, determines the distribution of head poses of the participants. Our results show that when the head pose distributions are significantly different in the train and test sets, the performance of the vision-based VAD models drops significantly. This suggests that previously reported results on datasets with a fixed physical configuration may overestimate the generalization capabilities of this type of models. We also propose a number of possible remedies to the spatial bias, including data augmentation, input masking and dynamic features, and provide an in-depth analysis of the visual cues used by the developed vision-based VAD models.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1109/ICPR48806.2021.9413345</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stefanov, Kalin and Adiban, Mohammad and Salvi, Giampiero}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Pattern Recognition}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Spatial Bias in Vision-Based Voice Activity Detection}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10433--10440}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICPR48806.2021.9413345}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1051-4651}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{ICPR48806.2021.9413345.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.18653/v1/2021.clpsych-1.13" class="col-sm-8">
        <!-- Title -->
        <div class="title">Analysis of Behavior Classification in Motivational Interviewing</div>
        <!-- Author -->
        <div class="author">
            

            Leili Tavabi, Trang Tran, <strong>Kalin Stefanov</strong>, Brian Borsari, Joshua Woolley, Stefan Scherer, and Mohammad Soleymani</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access</em>, 2021
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/2021.clpsych-1.13.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>Analysis of client and therapist behavior in counseling sessions can provide helpful insights for assessing the quality of the session and consequently, the client’s behavioral outcome. In this paper, we study the automatic classification of standardized behavior codes (annotations) used for assessment of psychotherapy sessions in Motivational Interviewing (MI). We develop models and examine the classification of client behaviors throughout MI sessions, comparing the performance by models trained on large pretrained embeddings (RoBERTa) versus interpretable and expert-selected features (LIWC). Our best performing model using the pretrained RoBERTa embeddings beats the baseline model, achieving an F1 score of 0.66 in the subject-independent 3-class classification. Through statistical analysis on the classification results, we identify prominent LIWC features that may not have been captured by the model using pretrained embeddings. Although classification using LIWC features underperforms RoBERTa, our findings motivate the future direction of incorporating auxiliary tasks in the classification of MI codes.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.18653/v1/2021.clpsych-1.13</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Analysis of Behavior Classification in Motivational Interviewing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tavabi, Leili and Tran, Trang and Stefanov, Kalin and Borsari, Brian and Woolley, Joshua and Scherer, Stefan and Soleymani, Mohammad}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.clpsych-1.13}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2021.clpsych-1.13}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{110--115}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{2021.clpsych-1.13.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1145/3474085.3479213" class="col-sm-8">
        <!-- Title -->
        <div class="title">Group-Level Focus of Visual Attention for Improved Next Speaker Prediction</div>
        <!-- Author -->
        <div class="author">
            

            Chris Birmingham, <strong>Kalin Stefanov</strong>, and Maja Mataric</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the ACM International Conference on Multimedia</em>, 2021
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/3474085.3479213.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>In this work we address the Next Speaker Prediction sub challenge of the ACM ’21 MultiMediate Grand Challenge. This challenge poses the problem of turn taking prediction in physically situated multiparty interaction. Solving this problem is essential for enabling fluent real-time multiparty human-machine interaction. This problem is made more difficult by the need for a robust solution that can perform effectively across a wide variety of settings and contexts. Prior work has shown that current state-of-the-art methods rely on machine learning approaches that do not generalize well to new settings and feature distributions. To address this problem, we propose the use of group-level focus of visual attention as additional information. We show that a simple combination of group-level focus of visual attention features and publicly available audio-video synchronizer models is competitive with state-of-the-art methods fine-tuned for the challenge dataset.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3474085.3479213</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Birmingham, Chris and Stefanov, Kalin and Mataric, Maja}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Group-Level Focus of Visual Attention for Improved Next Speaker Prediction}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450386517}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3474085.3479213}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3474085.3479213}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ACM International Conference on Multimedia}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4838--4842}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{focus of visual attention, neural networks, turn taking prediction}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Virtual Event, China}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{MM '21}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{3474085.3479213.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1145/3461615.3485430" class="col-sm-8">
        <!-- Title -->
        <div class="title">Group-Level Focus of Visual Attention for Improved Active Speaker Detection</div>
        <!-- Author -->
        <div class="author">
            

            Christopher Birmingham, Maja Mataric, and <strong>Kalin Stefanov</strong>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Companion Publication of the ACM International Conference on Multimodal Interaction</em>, 2021
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/3461615.3485430.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>This work addresses the problem of active speaker detection in physically situated multiparty interactions. This challenge requires a robust solution that can perform effectively across a wide range of speakers and physical contexts. Current state-of-the-art active speaker detection approaches rely on machine learning methods that do not generalize well to new physical settings. We find that these methods do not transfer well even between similar datasets. We propose the use of group-level focus of visual attention in combination with a general audio-video synchronizer method for improved active speaker detection across speakers and physical contexts. Our dataset-independent experiments demonstrate that the proposed approach outperforms state-of-the-art methods trained specifically for the task of active speaker detection.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3461615.3485430</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Birmingham, Christopher and Mataric, Maja and Stefanov, Kalin}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Group-Level Focus of Visual Attention for Improved Active Speaker Detection}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450384711}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3461615.3485430}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3461615.3485430}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Companion Publication of the ACM International Conference on Multimodal Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{37--42}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{neural networks, focus of visual attention, active speaker detection}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Montreal, QC, Canada}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{ICMI '21 Companion}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{3461615.3485430.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1109/TCDS.2019.2927941" class="col-sm-8">
        <!-- Title -->
        <div class="title">Self-Supervised Vision-Based Detection of the Active Speaker as Support for Socially Aware Language Acquisition</div>
        <!-- Author -->
        <div class="author">
            

            <strong>Kalin Stefanov</strong>, Jonas Beskow, and Giampiero Salvi</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>IEEE Transactions on Cognitive and Developmental Systems</em>, 2020
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/TCDS.2019.2927941" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>This paper presents a self-supervised method for visual detection of the active speaker in a multiperson spoken interaction scenario. Active speaker detection is a fundamental prerequisite for any artificial cognitive system attempting to acquire language in social settings. The proposed method is intended to complement the acoustic detection of the active speaker, thus improving the system robustness in noisy conditions. The method can detect an arbitrary number of possibly overlapping active speakers based exclusively on visual information about their face. Furthermore, the method does not rely on external annotations, thus complying with cognitive development. Instead, the method uses information from the auditory modality to support learning in the visual domain. This paper reports an extensive evaluation of the proposed method using a large multiperson face-to-face interaction data set. The results show good performance in a speaker dependent setting. However, in a speaker independent setting the proposed method yields a significantly lower performance. We believe that the proposed method represents an essential component of any artificial cognitive system or robotic platform engaging in social interactions.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1109/TCDS.2019.2927941</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stefanov, Kalin and Beskow, Jonas and Salvi, Giampiero}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Cognitive and Developmental Systems}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Self-Supervised Vision-Based Detection of the Active Speaker as Support for Socially Aware Language Acquisition}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{250--259}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TCDS.2019.2927941}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2379-8939}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{TCDS.2019.2927941}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1109/FG47880.2020.00123" class="col-sm-8">
        <!-- Title -->
        <div class="title">Emotion or Expressivity? An Automated Analysis of Nonverbal Perception in a Social Dilemma</div>
        <!-- Author -->
        <div class="author">
            

            Su Lei, <strong>Kalin Stefanov</strong>, and Jonathan Gratch</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition</em>, 2020
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/FG47880.2020.00123.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>An extensive body of research has examined how specific emotional expressions shape social perceptions and social decisions, yet recent scholarship in emotion research has raised questions about the validity of emotion as a construct. In this article, we contrast the value of measuring emotional expressions with the more general construct of expressivity (in the sense of conveying a thought or emotion through any nonverbal behavior) and develop models that can automatically extract perceived expressivity from videos. Although less extensive, a solid body of research has shown expressivity to be an important element when studying interpersonal perception, particularly in psychiatric contexts. Here we examine the role expressivity plays in predicting social perceptions and decisions in the context of a social dilemma. We show that perceivers use more than facial expressions when making judgments of expressivity and see these expressions as conveying thoughts as well as emotions (although facial expressions and emotional attributions explain most of the variance in these judgments). We next show that expressivity can be predicted with high accuracy using Lasso and random forests. Our analysis shows that features related to motion dynamics are particularly important for modeling these judgments. We also show that learned models of expressivity have value in recognizing important aspects of a social situation. First, we revisit a previously published finding which showed that smile intensity was associated with the unexpectedness of outcomes in social dilemmas; instead, we show that expressivity is a better predictor (and explanation) of this finding. Second, we provide preliminary evidence that expressivity is useful for identifying “moments of interest” in a video sequence.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1109/FG47880.2020.00123</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lei, Su and Stefanov, Kalin and Gratch, Jonathan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Emotion or Expressivity? An Automated Analysis of Nonverbal Perception in a Social Dilemma}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{544--551}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/FG47880.2020.00123}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{FG47880.2020.00123.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1145/3382507.3418853" class="col-sm-8">
        <!-- Title -->
        <div class="title">Multimodal Automatic Coding of Client Behavior in Motivational Interviewing</div>
        <!-- Author -->
        <div class="author">
            

            Leili Tavabi, <strong>Kalin Stefanov</strong>, Larry Zhang, Brian Borsari, Joshua D. Woolley, Stefan Scherer, and Mohammad Soleymani</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the ACM International Conference on Multimodal Interaction</em>, 2020
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/3382507.3418853.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>Motivational Interviewing (MI) is defined as a collaborative conversation style that evokes the client’s own intrinsic reasons for behavioral change. In MI research, the clients’ attitude (willingness or resistance) toward change as expressed through language, has been identified as an important indicator of their subsequent behavior change. Automated coding of these indicators provides systematic and efficient means for the analysis and assessment of MI therapy sessions. In this paper, we study and analyze behavioral cues in client language and speech that bear indications of the client’s behavior toward change during a therapy session, using a database of dyadic motivational interviews between therapists and clients with alcohol-related problems. Deep language and voice encoders, ie BERT and VGGish, trained on large amounts of data are used to extract features from each utterance. We develop a neural network to automatically detect the MI codes using both the clients’ and therapists’ language and clients’ voice, and demonstrate the importance of semantic context in such detection. Additionally, we develop machine learning models for predicting alcohol-use behavioral outcomes of clients through language and voice analysis. Our analysis demonstrates that we are able to estimate MI codes using clients’ textual utterances along with preceding textual context from both the therapist and client, reaching an F1-score of 0.72 for a speaker-independent three-class classification. We also report initial results for using the clients’ data for predicting behavioral outcomes, which outlines the direction for future work.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3382507.3418853</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tavabi, Leili and Stefanov, Kalin and Zhang, Larry and Borsari, Brian and Woolley, Joshua D. and Scherer, Stefan and Soleymani, Mohammad}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multimodal Automatic Coding of Client Behavior in Motivational Interviewing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450375818}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3382507.3418853}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3382507.3418853}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ACM International Conference on Multimodal Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{406--413}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{mental health, machine learning, motivational interviewing, human behavior}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Virtual Event, Netherlands}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{ICMI '20}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{3382507.3418853.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1145/3382507.3418832" class="col-sm-8">
        <!-- Title -->
        <div class="title">OpenSense: A Platform for Multimodal Data Acquisition and Behavior Perception</div>
        <!-- Author -->
        <div class="author">
            

            <strong>Kalin Stefanov</strong>, Baiyu Huang, Zongjian Li, and Mohammad Soleymani</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the ACM International Conference on Multimodal Interaction</em>, 2020
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/3382507.3418832.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
                <a href="https://github.com/intelligent-human-perception-laboratory/OpenSense" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>Automatic multimodal acquisition and understanding of social signals is an essential building block for natural and effective human-machine collaboration and communication. This paper introduces OpenSense, a platform for real-time multimodal acquisition and recognition of social signals. OpenSense enables precisely synchronized and coordinated acquisition and processing of human behavioral signals. Powered by the Microsoft’s Platform for Situated Intelligence, OpenSense supports a range of sensor devices and machine learning tools and encourages developers to add new components to the system through straightforward mechanisms for component integration. This platform also offers an intuitive graphical user interface to build application pipelines from existing components. OpenSense is freely available for academic research.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3382507.3418832</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stefanov, Kalin and Huang, Baiyu and Li, Zongjian and Soleymani, Mohammad}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{OpenSense: A Platform for Multimodal Data Acquisition and Behavior Perception}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450375818}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3382507.3418832}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3382507.3418832}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ACM International Conference on Multimodal Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{660--664}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{platform, perception, multimodal, behavior, open source}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Virtual Event, Netherlands}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{ICMI '20}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{3382507.3418832.pdf}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/intelligent-human-perception-laboratory/OpenSense}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
</ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography">
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1145/3323231" class="col-sm-8">
        <!-- Title -->
        <div class="title">Modeling of Human Visual Attention in Multiparty Open-World Dialogues</div>
        <!-- Author -->
        <div class="author">
            

            <strong>Kalin Stefanov</strong>, Giampiero Salvi, Dimosthenis Kontogiorgos, Hedvig Kjellström, and Jonas Beskow</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>ACM Transactions on Human-Robot Interaction</em>, 2019
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/3323231.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>This study proposes, develops, and evaluates methods for modeling the eye-gaze direction and head orientation of a person in multiparty open-world dialogues, as a function of low-level communicative signals generated by his/hers interlocutors. These signals include speech activity, eye-gaze direction, and head orientation, all of which can be estimated in real time during the interaction. By utilizing these signals and novel data representations suitable for the task and context, the developed methods can generate plausible candidate gaze targets in real time. The methods are based on Feedforward Neural Networks and Long Short-Term Memory Networks. The proposed methods are developed using several hours of unrestricted interaction data and their performance is compared with a heuristic baseline method. The study offers an extensive evaluation of the proposed methods that investigates the contribution of different predictors to the accurate generation of candidate gaze targets. The results show that the methods can accurately generate candidate gaze targets when the person being modeled is in a listening state. However, when the person being modeled is in a speaking state, the proposed methods yield significantly lower performance.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1145/3323231</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stefanov, Kalin and Salvi, Giampiero and Kontogiorgos, Dimosthenis and Kjellstr\"{o}m, Hedvig and Beskow, Jonas}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Modeling of Human Visual Attention in Multiparty Open-World Dialogues}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">issue_date</span> <span class="p">=</span> <span class="s">{June 2019}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3323231}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3323231}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Human-Robot Interaction}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{21}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{multiparty, eye-gaze direction, open-world dialogue, head orientation, human-human interaction}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{3323231.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1145/3349537.3352794" class="col-sm-8">
        <!-- Title -->
        <div class="title">Towards Digitally-Mediated Sign Language Communication</div>
        <!-- Author -->
        <div class="author">
            

            <strong>Kalin Stefanov</strong>, and Mayumi Bono</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the International Conference on Human-Agent Interaction</em>, 2019
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/3349537.3352794.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>This paper presents our efforts towards building an architecture for digitally-mediated sign language communication. The architecture is based on a client-server model and enables a near real-time recognition of sign language signs on a mobile device. The paper describes the two main components of the architecture, a recognition engine (server-side) and a mobile application (client-side), and outlines directions for future work.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3349537.3352794</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stefanov, Kalin and Bono, Mayumi}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Digitally-Mediated Sign Language Communication}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450369220}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3349537.3352794}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3349537.3352794}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Human-Agent Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{286--288}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{sign language, key word signing, recognition}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Kyoto, Japan}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{HAI '19}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{3349537.3352794.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1145/3340555.3353750" class="col-sm-8">
        <!-- Title -->
        <div class="title">Multimodal Learning for Identifying Opportunities for Empathetic Responses</div>
        <!-- Author -->
        <div class="author">
            

            Leili Tavabi, <strong>Kalin Stefanov</strong>, Setareh Nasihati Gilani, David Traum, and Mohammad Soleymani</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the ACM International Conference on Multimodal Interaction</em>, 2019
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/3340555.3353750.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>Embodied interactive agents possessing emotional intelligence and empathy can create natural and engaging social interactions. Providing appropriate responses by interactive virtual agents requires the ability to perceive users’ emotional states. In this paper, we study and analyze behavioral cues that indicate an opportunity to provide an empathetic response. Emotional tone in language in addition to facial expressions are strong indicators of dramatic sentiment in conversation that warrant an empathetic response. To automatically recognize such instances, we develop a multimodal deep neural network for identifying opportunities when the agent should express positive or negative empathetic responses. We train and evaluate our model using audio, video and language from human-agent interactions in a wizard-of-Oz setting, using the wizard’s empathetic responses and annotations collected on Amazon Mechanical Turk as ground-truth labels. Our model outperforms a text-based baseline achieving F1-score of 0.71 on a three-class classification. We further investigate the results and evaluate the capability of such a model to be deployed for real-world human-agent interactions.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3340555.3353750</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tavabi, Leili and Stefanov, Kalin and Nasihati Gilani, Setareh and Traum, David and Soleymani, Mohammad}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multimodal Learning for Identifying Opportunities for Empathetic Responses}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450368605}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3340555.3353750}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3340555.3353750}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ACM International Conference on Multimodal Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{95--104}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{virtual human, machine learning, multimodal sentiment, empathy, human behavior}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Suzhou, China}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{ICMI '19}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{3340555.3353750.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1145/3340555.3353737" class="col-sm-8">
        <!-- Title -->
        <div class="title">Multimodal Analysis and Estimation of Intimate Self-Disclosure</div>
        <!-- Author -->
        <div class="author">
            

            Mohammad Soleymani, <strong>Kalin Stefanov</strong>, Sin-Hwa Kang, Jan Ondras, and Jonathan Gratch</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the ACM International Conference on Multimodal Interaction</em>, 2019
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/3340555.3353737.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>Self-disclosure to others has a proven benefit for one’s mental health. It is shown that disclosure to computers can be similarly beneficial for emotional and psychological well-being. In this paper, we analyzed verbal and nonverbal behavior associated with self-disclosure in two datasets containing structured human-human and human-agent interviews from more than 200 participants. Correlation analysis of verbal and nonverbal behavior revealed that linguistic features such as affective and cognitive content in verbal behavior, and nonverbal behavior such as head gestures are associated with intimate self-disclosure. A multimodal deep neural network was developed to automatically estimate the level of intimate self-disclosure from verbal and nonverbal behavior. Between modalities, verbal behavior was the best modality for estimating self-disclosure within-corpora achieving r = 0.66. However, the cross-corpus evaluation demonstrated that nonverbal behavior can outperform language modality in cross-corpus evaluation. Such automatic models can be deployed in interactive virtual agents or social robots to evaluate rapport and guide their conversational strategy.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3340555.3353737</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Soleymani, Mohammad and Stefanov, Kalin and Kang, Sin-Hwa and Ondras, Jan and Gratch, Jonathan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multimodal Analysis and Estimation of Intimate Self-Disclosure}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450368605}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3340555.3353737}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3340555.3353737}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ACM International Conference on Multimodal Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{59--68}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{neural networks, nonverbal behavior, self-disclosure, natural language understanding}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Suzhou, China}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{ICMI '19}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{3340555.3353737.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
</ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="stefanov2018recognition" class="col-sm-8">
        <!-- Title -->
        <div class="title">Recognition and Generation of Communicative Signals: Modeling of Hand Gestures, Speech Activity and Eye-Gaze in Human-Machine Interaction</div>
        <!-- Author -->
        <div class="author">
            

            <strong>Kalin Stefanov</strong>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>KTH Royal Institute of Technology</em>, 2018
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/phd.thesis.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>Nonverbal communication is essential for natural and effective face-to-face human-human interaction. It is the process of communicating through sending and receiving wordless (mostly visual, but also auditory) signals between people. Consequently, a natural and effective face-to-face human-machine interaction requires machines (e.g., robots) to understand and produce such human-like signals. There are many types of nonverbal signals used in this form of communication including, body postures, hand gestures, facial expressions, eye movements, touches and uses of space. This thesis investigates two of these nonverbal signals: hand gestures and eye-gaze. The main goal of the thesis is to propose computational methods for real-time recognition and generation of these two signals in order to facilitate natural and effective human-machine interaction. The first topic addressed in the thesis is the real-time recognition of hand gestures and its application to recognition of isolated sign language signs. Hand gestures can also provide important cues during human-robot interaction, for example, emblems are type of hand gestures with specific meaning used to substitute spoken words. The thesis has two main contributions with respect to the recognition of hand gestures: 1) a newly collected dataset of isolated Swedish Sign Language signs, and 2) a real-time hand gestures recognition method. The second topic addressed in the thesis is the general problem of real-time speech activity detection in noisy and dynamic environments and its application to socially-aware language acquisition. Speech activity can also provide important information during human-robot interaction, for example, the current active speaker’s hand gestures and eye-gaze direction or head orientation can play an important role in understanding the state of the interaction. The thesis has one main contribution with respect to speech activity detection: a real-time vision-based speech activity detection method. The third topic addressed in the thesis is the real-time generation of eye-gaze direction or head orientation and its application to human-robot interaction. Eye-gaze direction or head orientation can provide important cues during human-robot interaction, for example, it can regulate who is allowed to speak when and coordinate the changes in the roles on the conversational floor (e.g., speaker, addressee, and bystander). The thesis has two main contributions with respect to the generation of eye-gaze direction or head orientation: 1) a newly collected dataset of face-to-face interactions, and 2) a real-time eye-gaze direction or head orientation generation method.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">stefanov2018recognition</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Recognition and Generation of Communicative Signals: Modeling of Hand Gestures, Speech Activity and Eye-Gaze in Human-Machine Interaction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stefanov, Kalin}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{KTH Royal Institute of Technology}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{phd.thesis.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography">
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.21437/GLU.2017-10" class="col-sm-8">
        <!-- Title -->
        <div class="title">Vision-based Active Speaker Detection in Multiparty Interaction</div>
        <!-- Author -->
        <div class="author">
            

            <strong>Kalin Stefanov</strong>, Jonas Beskow, and Giampiero Salvi</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the International Workshop on Grounding Language Understanding</em>, 2017
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/GLU.2017-10.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>This paper presents a supervised learning method for automatic visual detection of the active speaker in multiparty interactions. The presented detectors are built using a multimodal multiparty interaction dataset previously recorded with the purpose to explore patterns in the focus of visual attention of humans. Three different conditions are included: two humans involved in taskbased interaction with a robot; the same two humans involved in task-based interaction where the robot is replaced by a third human, and a free three-party human interaction. The paper also presents an evaluation of the active speaker detection method in a speaker dependent experiment showing that the method achieves good accuracy rates in a fairly unconstrained scenario using only image data as input. The main goal of the presented method is to provide real-time detection of the active speaker within a broader framework implemented on a robot and used to generate natural focus of visual attention behavior during multiparty human-robot interactions.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.21437/GLU.2017-10</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stefanov, Kalin and Beskow, Jonas and Salvi, Giampiero}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Vision-based Active Speaker Detection in Multiparty Interaction}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Workshop on Grounding Language Understanding}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{47--51}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.21437/GLU.2017-10}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{GLU.2017-10.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="ecp17141004" class="col-sm-8">
        <!-- Title -->
        <div class="title">A Real-time Gesture Recognition System for Isolated Swedish Sign Language Signs</div>
        <!-- Author -->
        <div class="author">
            

            <strong>Kalin Stefanov</strong>, and Jonas Beskow</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the European and Nordic Symposium on Multimodal Communication</em>, 2017
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/ecp17141004.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>This paper describes a method for automatic recognition of isolated Swedish Sign Language signs for the purpose of educational signing-based games. Two datasets consisting of 51 signs have been recorded from a total of 7 (experienced) and 10 (inexperienced) adult signers. The signers performed all of the signs 5 times and were captured with a RGB-D (Kinect) sensor, via a purpose-built recording application. A recognizer based on manual components of sign language is presented and tested on the collected datasets. Signer-dependent recognition rate is 95.3% for the most consistent signer. Signer-independent recognition rate is on average 57.9% for the experienced signers and 68.9% for the inexperienced.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ecp17141004</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Real-time Gesture Recognition System for Isolated Swedish Sign Language Signs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stefanov, Kalin and Beskow, Jonas}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Link{\"o}ping Electronic Conference Proceedings}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Link{\"o}ping University Electronic Press}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{18--27}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the European and Nordic Symposium on Multimodal Communication}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{ecp17141004.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
</ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography">
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="L16-1703" class="col-sm-8">
        <!-- Title -->
        <div class="title">A Multi-party Multi-modal Dataset for Focus of Visual Attention in Human-human and Human-robot Interaction</div>
        <!-- Author -->
        <div class="author">
            

            <strong>Kalin Stefanov</strong>, and Jonas Beskow</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the International Conference on Language Resources and Evaluation</em>, 2016
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/L16-1703.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>This papers describes a data collection setup and a newly recorded dataset. The main purpose of this dataset is to explore patterns in the focus of visual attention of humans under three different conditions - two humans involved in task-based interaction with a robot; same two humans involved in task-based interaction where the robot is replaced by a third human, and a free three-party human interaction. The dataset contains two parts - 6 sessions with duration of approximately 3 hours and 9 sessions with duration of approximately 4.5 hours. Both parts of the dataset are rich in modalities and recorded data streams - they include the streams of three Kinect v2 devices (color, depth, infrared, body and face data), three high quality audio streams, three high resolution GoPro video streams, touch data for the task-based interactions and the system state of the robot. In addition, the second part of the dataset introduces the data streams from three Tobii Pro Glasses 2 eye trackers. The language of all interactions is English and all data streams are spatially and temporally aligned.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">L16-1703</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Multi-party Multi-modal Dataset for Focus of Visual Attention in Human-human and Human-robot Interaction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stefanov, Kalin and Beskow, Jonas}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Language Resources and Evaluation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Portoro{\v{z}}, Slovenia}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{European Language Resources Association}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/L16-1703}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4440--4444}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{L16-1703.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1145/3005467.3005470" class="col-sm-8">
        <!-- Title -->
        <div class="title">Look Who’s Talking: Visual Identification of the Active Speaker in Multi-Party Human-Robot Interaction</div>
        <!-- Author -->
        <div class="author">
            

            <strong>Kalin Stefanov</strong>, Akihiro Sugimoto, and Jonas Beskow</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the International Workshop on Advancements in Social Signal Processing for Multimodal Interaction</em>, 2016
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/3005467.3005470.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>This paper presents analysis of a previously recorded multi-modal interaction dataset. The primary purpose of that dataset is to explore patterns in the focus of visual attention of humans under three different conditions - two humans involved in task-based interaction with a robot; the same two humans involved in task-based interaction where the robot is replaced by a third human, and a free three-party human interaction. The paper presents a data-driven methodology for automatic visual identification of the active speaker based on facial action units (AUs). The paper also presents an evaluation of the proposed methodology on 12 different interactions with an approximate length of 4 hours. The methodology will be implemented on a robot and used to generate natural focus of visual attention behavior during multi-party human-robot interactions.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3005467.3005470</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stefanov, Kalin and Sugimoto, Akihiro and Beskow, Jonas}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Look Who's Talking: Visual Identification of the Active Speaker in Multi-Party Human-Robot Interaction}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450345576}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3005467.3005470}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3005467.3005470}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Workshop on Advancements in Social Signal Processing for Multimodal Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{22--27}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{human-robot interaction, active speaker identification, multi-modal interaction}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Tokyo, Japan}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{ASSP4MI '16}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{3005467.3005470.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="stefanov2016gesture" class="col-sm-8">
        <!-- Title -->
        <div class="title">Gesture Recognition System for Isolated Sign Language Signs</div>
        <!-- Author -->
        <div class="author">
            

            <strong>Kalin Stefanov</strong>, and Jonas Beskow</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the European and Nordic Symposium on Multimodal Communication</em>, 2016
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/stefanov2016gesture" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>This paper describes a method for automatic recognition of isolated Swedish Sign Language signs for the purpose of educational signing-based games. Two datasets consisting of 51 signs have been recorded from a total of 7 (experienced) and 10 (inexperienced) adult signers. The signers performed all of the signs 5 times and were captured with a RGB-D (Kinect) sensor, via a purpose-built recording application. A recognizer based on manual components of sign language is presented and tested on the collected datasets. Signer-dependent recognition rate is 95.3% for the most consistent signer. Signer-independent recognition rate is on average 57.9% for the experienced signers and 68.9% for the inexperienced.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">stefanov2016gesture</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Gesture Recognition System for Isolated Sign Language Signs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stefanov, Kalin and Beskow, Jonas}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the European and Nordic Symposium on Multimodal Communication}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{57--59}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{stefanov2016gesture}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
</ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1145/2818346.2823294" class="col-sm-8">
        <!-- Title -->
        <div class="title">Public Speaking Training with a Multimodal Interactive Virtual Audience Framework</div>
        <!-- Author -->
        <div class="author">
            

            Mathieu Chollet, <strong>Kalin Stefanov</strong>, Helmut Prendinger, and Stefan Scherer</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the ACM International Conference on Multimodal Interaction</em>, 2015
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/2818346.2823294.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>We have developed an interactive virtual audience platform for public speaking training. Users’ public speaking behavior is automatically analyzed using multimodal sensors, and ultimodal feedback is produced by virtual characters and generic visual widgets depending on the user’s behavior. The flexibility of our system allows to compare different interaction mediums (e.g. virtual reality vs normal interaction), social situations (e.g. one-on-one meetings vs large audiences) and trained behaviors (e.g. general public speaking performance vs specific behaviors).</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/2818346.2823294</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chollet, Mathieu and Stefanov, Kalin and Prendinger, Helmut and Scherer, Stefan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Public Speaking Training with a Multimodal Interactive Virtual Audience Framework}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450339124}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/2818346.2823294}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2818346.2823294}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ACM International Conference on Multimodal Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{367--368}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{automatic behavior recognition, public speaking training, virtual audience}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Seattle, Washington, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{ICMI '15}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{2818346.2823294.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li></ol>

  <h2 class="year">2014</h2>
  <ol class="bibliography">
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1007/978-3-642-55143-7_4" class="col-sm-8">
        <!-- Title -->
        <div class="title">Tutoring Robots</div>
        <!-- Author -->
        <div class="author">
            

            Samer Al Moubayed, Jonas Beskow, Bajibabu Bollepalli, Ahmed Hussen-Abdelaziz, Martin Johansson, Maria Koutsombogera, José David Lopes, Jekaterina Novikova, Catharine Oertel, Gabriel Skantze, <strong>Kalin Stefanov</strong>, and Gül Varol</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Innovative and Creative Developments in Multimodal Interaction Systems</em>, 2014
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/978-3-642-55143-7_4.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>This project explores a novel experimental setup towards building spoken, multi-modally rich, and human-like multiparty tutoring agent. A setup is developed and a corpus is collected that targets the development of a dialogue system platform to explore verbal and nonverbal tutoring strategies in multiparty spoken interactions with embodied agents. The dialogue task is centered on two participants involved in a dialogue aiming to solve a card-ordering game. With the participants sits a tutor that helps the participants perform the task and organizes and balances their interaction. Different multimodal signals captured and auto-synchronized by different audio-visual capture technologies were coupled with manual annotations to build a situated model of the interaction based on the participants personalities, their temporally-changing state of attention, their conversational engagement and verbal dominance, and the way these are correlated with the verbal and visual feedback, turn-management, and conversation regulatory actions generated by the tutor. At the end of this chapter we discuss the potential areas of research and developments this work opens and some of the challenges that lie in the road ahead.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1007/978-3-642-55143-7_4</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Al Moubayed, Samer and Beskow, Jonas and Bollepalli, Bajibabu and Hussen-Abdelaziz, Ahmed and Johansson, Martin and Koutsombogera, Maria and Lopes, Jos{\'e} David and Novikova, Jekaterina and Oertel, Catharine and Skantze, Gabriel and Stefanov, Kalin and Varol, G{\"u}l}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Rybarczyk, Yves and Cardoso, Tiago and Rosas, Jo{\~a}o and Camarinha-Matos, Luis M.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tutoring Robots}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Innovative and Creative Developments in Multimodal Interaction Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Berlin Heidelberg}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Berlin, Heidelberg}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{80--113}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-642-55143-7}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{978-3-642-55143-7_4.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="8542606" class="col-sm-8">
        <!-- Title -->
        <div class="title">Human-Robot Collaborative Tutoring using Multiparty Multimodal Spoken Dialogue</div>
        <!-- Author -->
        <div class="author">
            

            Samer Al Moubayed, Jonas Beskow, Bajibabu Bollepalli, Joakim Gustafson, Ahmed Hussen-Abdelaziz, Martin Johansson, Maria Koutsombogera, José David Lopes, Jekaterina Novikova, Catharine Oertel, Gabriel Skantze, <strong>Kalin Stefanov</strong>, and Gül Varol</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction</em>, 2014
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/8542606.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>In this paper, we describe a project that explores a novel experimental setup towards building a spoken, multi-modally rich, and human-like multiparty tutoring robot. A human-robot interaction setup is designed, and a human-human dialogue corpus is collected. The corpus targets the development of a dialogue system platform to study verbal and nonverbal tutoring strategies in multiparty spoken interactions with robots which are capable of spoken dialogue. The dialogue task is centered on two participants involved in a dialogue aiming to solve a card-ordering game. Along with the participants sits a tutor (robot) that helps the participants perform the task, and organizes and balances their interaction. Different multimodal signals captured and autosynchronized by different audio-visual capture technologies, such as a microphone array, Kinects, and video cameras, were coupled with manual annotations. These are used build a situated model of the interaction based on the participants personalities, their state of attention, their conversational engagement and verbal dominance, and how that is correlated with the verbal and visual feedback, turn-management, and conversation regulatory actions generated by the tutor. Driven by the analysis of the corpus, we will show also the detailed design methodologies for an affective, and multimodally rich dialogue system that allows the robot to measure incrementally the attention states, and the dominance for each participant, allowing the robot head Furhat to maintain a wellcoordinated, balanced, and engaging conversation, that attempts to maximize the agreement and the contribution to solve the task. This project sets the first steps to explore the potential of using multimodal dialogue systems to build interactive robots that can serve in educational, team building, and collaborative task solving applications.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">8542606</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Moubayed, Samer Al and Beskow, Jonas and Bollepalli, Bajibabu and Gustafson, Joakim and Hussen-Abdelaziz, Ahmed and Johansson, Martin and Koutsombogera, Maria and Lopes, José David and Novikova, Jekaterina and Oertel, Catharine and Skantze, Gabriel and Stefanov, Kalin and Varol, Gül}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Human-Robot Collaborative Tutoring using Multiparty Multimodal Spoken Dialogue}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{112--113}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2167-2121}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{8542606.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="L14-1641" class="col-sm-8">
        <!-- Title -->
        <div class="title">The Tutorbot Corpus — A Corpus for Studying Tutoring Behaviour in Multiparty Face-to-Face Spoken Dialogue</div>
        <!-- Author -->
        <div class="author">
            

            Maria Koutsombogera, Samer Al Moubayed, Bajibabu Bollepalli, Ahmed Hussen Abdelaziz, Martin Johansson, José David Aguas Lopes, Jekaterina Novikova, Catharine Oertel, <strong>Kalin Stefanov</strong>, and Gül Varol</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the International Conference on Language Resources and Evaluation</em>, 2014
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/L14-1641.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>This paper describes a novel experimental setup exploiting state-of-the-art capture equipment to collect a multimodally rich game-solving collaborative multiparty dialogue corpus. The corpus is targeted and designed towards the development of a dialogue system platform to explore verbal and nonverbal tutoring strategies in multiparty spoken interactions. The dialogue task is centered on two participants involved in a dialogue aiming to solve a card-ordering game. The participants were paired into teams based on their degree of extraversion as resulted from a personality test. With the participants sits a tutor that helps them perform the task, organizes and balances their interaction and whose behavior was assessed by the participants after each interaction. Different multimodal signals captured and auto-synchronized by different audio-visual capture technologies, together with manual annotations of the tutors behavior constitute the Tutorbot corpus. This corpus is exploited to build a situated model of the interaction based on the participants temporally-changing state of attention, their conversational engagement and verbal dominance, and their correlation with the verbal and visual feedback and conversation regulatory actions generated by the tutor.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">L14-1641</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Tutorbot Corpus {---} A Corpus for Studying Tutoring Behaviour in Multiparty Face-to-Face Spoken Dialogue}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Koutsombogera, Maria and Moubayed, Samer Al and Bollepalli, Bajibabu and Abdelaziz, Ahmed Hussen and Johansson, Martin and Lopes, Jos{\'e} David Aguas and Novikova, Jekaterina and Oertel, Catharine and Stefanov, Kalin and Varol, G{\"u}l}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Language Resources and Evaluation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Reykjavik, Iceland}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{European Language Resources Association}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://www.lrec-conf.org/proceedings/lrec2014/pdf/832_Paper.pdf}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4196--4201}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{L14-1641.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="774962" class="col-sm-8">
        <!-- Title -->
        <div class="title">A Data-Driven Approach to Detection of Interruptions in Human-Human Conversations</div>
        <!-- Author -->
        <div class="author">
            

            Raveesh Meena, Saeed Dabbaghchian, and <strong>Kalin Stefanov</strong>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the FONETIK</em>, 2014
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/774962.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>We report the results of our initial efforts towards automatic detection of user’s interruptions in a spoken human–machine dialogue. In a first step, we explored the use of automatically extractable acoustic features, frequency and intensity, in discriminating listener’s interruptions in human–human conversations. A preliminary analysis of interaction snippets from the HCRC Map Task corpus suggests that for the task at hand, intensity is a stronger feature than frequency, and using intensity in combination with feature loudness offers the best results for a k-means clustering algorithm.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">774962</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Data-Driven Approach to Detection of Interruptions in Human-Human Conversations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Meena, Raveesh and Dabbaghchian, Saeed and Stefanov, Kalin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the FONETIK}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{29--32}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{774962.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="beskow2014tivoli" class="col-sm-8">
        <!-- Title -->
        <div class="title">Tivoli - Learning Signs Through Games and Interaction for Children with Communicative Disorders</div>
        <!-- Author -->
        <div class="author">
            

            Jonas Beskow, Simon Alexanderson, <strong>Kalin Stefanov</strong>, Britt Claesson, Sandra Derbring, Morgan Fredriksson, J. Starck, and E. Axelsson</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the Biennial Conference of the International Society for Augmentative and Alternative Communication</em>, 2014
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
        </div>
        

        <!-- Hidden abstract block -->
        <!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">beskow2014tivoli</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tivoli - Learning Signs Through Games and Interaction for Children with Communicative Disorders}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Beskow, Jonas and Alexanderson, Simon and Stefanov, Kalin and Claesson, Britt and Derbring, Sandra and Fredriksson, Morgan and Starck, J. and Axelsson, E.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Biennial Conference of the International Society for Augmentative and Alternative Communication}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
</ol>

  <h2 class="year">2013</h2>
  <ol class="bibliography">
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="e2e46e0abff94d4d944bec6ab0553087" class="col-sm-8">
        <!-- Title -->
        <div class="title">A Kinect Corpus of Swedish Sign Language Signs</div>
        <!-- Author -->
        <div class="author">
            

            <strong>Kalin Stefanov</strong>, and Jonas Beskow</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the Workshop on Multimodal Corpora: Beyond Audio and Video</em>, 2013
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/e2e46e0abff94d4d944bec6ab0553087.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>We describe a corpus of Swedish sign language signs, recorded for the purpose of an educational signing game. The primary target group of the game is children with communicative disabilities, and the goal is to offer a playful and interactive way of learning and practicing sign language signs to these children, as well as to their friends and family. As a first step, a dataset consisting of 51 signs has been recorded for a total of 10 adult signers. The signers performed all of the signs five times and were captured with an RGB-D (Microsoft Kinect) sensor, via a purpose-built recording application.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">e2e46e0abff94d4d944bec6ab0553087</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Kinect Corpus of Swedish Sign Language Signs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stefanov, Kalin and Beskow, Jonas}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2013}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Workshop on Multimodal Corpora: Beyond Audio and Video}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{e2e46e0abff94d4d944bec6ab0553087.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="beskow2013tivoli" class="col-sm-8">
        <!-- Title -->
        <div class="title">The Tivoli System–A Sign-driven Game for Children with Communicative Disorders</div>
        <!-- Author -->
        <div class="author">
            

            Jonas Beskow, Simon Alexanderson, <strong>Kalin Stefanov</strong>, Britt Claesson, Sandra Derbring, and Morgan Fredriksson</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the Symposium on Multimodal Communication</em>, 2013
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
        </div>
        

        <!-- Hidden abstract block -->
        <!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">beskow2013tivoli</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Tivoli System--A Sign-driven Game for Children with Communicative Disorders}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Beskow, Jonas and Alexanderson, Simon and Stefanov, Kalin and Claesson, Britt and Derbring, Sandra and Fredriksson, Morgan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Symposium on Multimodal Communication}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2013}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="3903" class="col-sm-8">
        <!-- Title -->
        <div class="title">Web-Enabled 3D Talking Avatars Based on WebGL and HTML5</div>
        <!-- Author -->
        <div class="author">
            

            Jonas Beskow, and <strong>Kalin Stefanov</strong>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the International Conference on Intelligent Virtual Agents</em>, 2013
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/3903.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>We describe a system for plugin-free deployment of 3D talking characters on the web. The system employs the WebGL capabilites of modern web browsers in order to produce real-time animation of speech movements, in synchrony with text-to-speech synthesis, played back using HTML5 audio functionalty. The implementation is divided into a client and a server part, where the server delivers the audio waveform and the animation tracks for lip synchronisation, and the client takes care of audio playback and rendering of the avatar in the browser.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">3903</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Web-Enabled 3D Talking Avatars Based on WebGL and HTML5}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Beskow, Jonas and Stefanov, Kalin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on Intelligent Virtual Agents}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{8108}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{486}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2013}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{3903.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
</ol>

  <h2 class="year">2012</h2>
  <ol class="bibliography">
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="10.1145/2388676.2388736" class="col-sm-8">
        <!-- Title -->
        <div class="title">Multimodal Multiparty Social Interaction with the Furhat Head</div>
        <!-- Author -->
        <div class="author">
            

            Samer Al Moubayed, Gabriel Skantze, Jonas Beskow, <strong>Kalin Stefanov</strong>, and Joakim Gustafson</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the ACM International Conference on Multimodal Interaction</em>, 2012
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/2388676.2388736.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>We will show in this demonstrator an advanced multimodal and multiparty spoken conversational system using Furhat, a robot head based on projected facial animation. Furhat is a human-like interface that utilizes facial animation for physical robot heads using back-projection. In the system, multimodality is enabled using speech and rich visual input signals such as multi-person real-time face tracking and microphone tracking. The demonstrator will showcase a system that is able to carry out social dialogue with multiple interlocutors simultaneously with rich output signals such as eye and head coordination, lips synchronized speech synthesis, and non-verbal facial gestures used to regulate fluent and expressive multiparty conversations.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/2388676.2388736</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Al Moubayed, Samer and Skantze, Gabriel and Beskow, Jonas and Stefanov, Kalin and Gustafson, Joakim}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multimodal Multiparty Social Interaction with the Furhat Head}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450314671}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/2388676.2388736}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/2388676.2388736}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the ACM International Conference on Multimodal Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{293--294}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{speech, spoken dialog, gaze, robot head, microphone tracking, gesture, facial animation, furhat, multiparty interaction, multimodal systems}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Santa Monica, California, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{ICMI '12}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{2388676.2388736.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
<li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="eyben2012socially" class="col-sm-8">
        <!-- Title -->
        <div class="title">Socially Aware Many-to-Machine Communication</div>
        <!-- Author -->
        <div class="author">
            

            F. Eyben, E. Gilmartin, C. Joder, E. Marchi, C. Munier, <strong>Kalin Stefanov</strong>, F. Weninger, and B. Schuller</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>In Proceedings of the International Summer Workshop on Multimodal Interfaces</em>, 2012
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>This reports describes the output of the project P5: Socially Aware Many-to-Machine Communication (M2M) at the eNTERFACE’12 workshop. In this project, we designed and implemented a new front-end for handling multi-user interaction in a dialog system. We exploit the Microsoft Kinect device for capturing multimodal input and extract some features describing user and face positions. These data are then analyzed in real-time to robustly detect speech and determine both who is speaking and whether the speech is directed to the system or not. This new front-end is integrated to the SEMAINE (Sustained Emotionally colored Machine-human Interaction using Nonverbal Expression) system. Furthermore, a multimodal corpus has been created, capturing all of the system inputs in two different scenarios involving human-human and human-computer interaction.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">eyben2012socially</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Socially Aware Many-to-Machine Communication}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Eyben, F. and Gilmartin, E. and Joder, C. and Marchi, E. and Munier, C. and Stefanov, Kalin and Weninger, F. and Schuller, B.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Summer Workshop on Multimodal Interfaces}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li>
</ol>

  <h2 class="year">2011</h2>
  <ol class="bibliography"><li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="grinberg2011d2" class="col-sm-8">
        <!-- Title -->
        <div class="title">D2. 4.3 Spreading Activation Components v3 - LarKC Project Deliverable</div>
        <!-- Author -->
        <div class="author">
            

            M. Grinberg, H. Stefanov, <strong>Kalin Stefanov</strong>, and Ivan Peikov</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em></em> 2011
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
        </div>
        

        <!-- Hidden abstract block -->
        <!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">grinberg2011d2</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{D2. 4.3 Spreading Activation Components v3 - LarKC Project Deliverable}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Grinberg, M. and Stefanov, H. and Stefanov, Kalin and Peikov, Ivan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2011}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li></ol>

  <h2 class="year">2010</h2>
  <ol class="bibliography"><li>
<div class="row">
    <div class="col-sm-2 preview"><img class="preview" src="/assets/img/paper.png"></div>

    <!-- Entry bib key -->
    <div id="1803.11088" class="col-sm-8">
        <!-- Title -->
        <div class="title">Webcam-based Eye Gaze Tracking under Natural Head Movement</div>
        <!-- Author -->
        <div class="author">
            

            <strong>Kalin Stefanov</strong>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
            <em>University of Amsterdam</em>, 2010
        </div>
        <div class="periodical">
            
        </div>

        <!-- Links/Buttons -->
        <div class="links">
                <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
                <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
                <a href="/assets/pdf/1803.11088.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        </div>
        

        <!-- Hidden abstract block -->
        <div class="abstract hidden">
            <p>This manuscript investigates and proposes a visual gaze tracker that tackles the problem using only an ordinary web camera and no prior knowledge in any sense (scene set-up, camera intrinsic and/or extrinsic parameters). The tracker we propose is based on the observation that our desire to grant the freedom of natural head movement to the user requires 3D modeling of the scene set-up. Although, using a single low resolution web camera bounds us in dimensions (no depth can be recovered), we propose ways to cope with this drawback and model the scene in front of the user. We tackle this three-dimensional problem by realizing that it can be viewed as series of two-dimensional special cases. Then, we propose a procedure that treats each movement of the user’s head as a special two-dimensional case, hence reducing the complexity of the problem back to two dimensions. Furthermore, the proposed tracker is calibration free and discards this tedious part of all previously mentioned trackers. Experimental results show that the proposed tracker achieves good results, given the restrictions on it. We can report that the tracker commits a mean error of (56.95, 70.82) pixels in x and y direction, respectively, when the user’s head is as static as possible (no chin-rests are used). Furthermore, we can report that the proposed tracker commits a mean error of (87.18, 103.86) pixels in x and y direction, respectively, under natural head movement.</p>
        </div>
<!-- Hidden bibtex block -->
        <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@mastersthesis</span><span class="p">{</span><span class="nl">1803.11088</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Stefanov, Kalin}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Webcam-based Eye Gaze Tracking under Natural Head Movement}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{University of Amsterdam}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2010}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{1803.11088}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{1803.11088.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">preview</span> <span class="p">=</span> <span class="s">{paper.png}</span>
<span class="p">}</span></code></pre></figure>
        </div>
    </div>
</div>
</li></ol>


</div>

    </article>
</div>

        </div>
        <!-- Footer -->
    <footer class="fixed-bottom">
        <div class="container mt-0">
<a href="mailto:%6B%61%6C%69%6E.%73%74%65%66%61%6E%6F%76@%6D%6F%6E%61%73%68.%65%64%75" title="email"><i class="fas fa-envelope"></i></a>
            
        
               <a href="https://research.monash.edu/en/persons/kalin-stefanov" title="work" rel="external nofollow noopener" target="_blank"><i class="fas fa-briefcase"></i></a>
            
        
               <a href="https://scholar.google.com/citations?user=0ZjgqkAAAAAJ" title="scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            
        
               <a href="https://www.linkedin.com/in/kmstefanov" title="linkedin" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            
        
               <a href="https://twitter.com/kmstefanov" title="twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a>
            
        </div>
    </footer>

        <!-- JavaScripts -->
        <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

        <!-- Bootsrap & MDB scripts -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

        
        
    <!-- Medium Zoom JS -->
    <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
    <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
<script defer src="/assets/js/common.js"></script>

        <!-- MathJax -->
    <script type="text/javascript">
        window.MathJax = { tex: { tags: 'ams' } };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
    <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

        
        
    <!-- Scrolling Progress Bar -->
    <script type="text/javascript">
        /*
        * This JavaScript code has been adapted from the article
        * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
        * published on the website https://css-tricks.com on the 7th of May, 2014.
        * Couple of changes were made to the original code to make it compatible
        * with the `al-foio` theme.
        */
        const progressBar = $("#progress");
        /*
        * We set up the bar after all elements are done loading.
        * In some cases, if the images in the page are larger than the intended
        * size they'll have on the page, they'll be resized via CSS to accomodate
        * the desired size. This mistake, however, breaks the computations as the
        * scroll size is computed as soon as the elements finish loading.
        * To account for this, a minimal delay was introduced before computing the
        * values.
        */
        window.onload = function () {
            setTimeout(progressBarSetup, 50);
        };
        /*
        * We set up the bar according to the browser.
        * If the browser supports the progress element we use that.
        * Otherwise, we resize the bar thru CSS styling
        */
        function progressBarSetup() {
            if ("max" in document.createElement("progress")) {
                initializeProgressElement();
                $(document).on("scroll", function() {
                    progressBar.attr({ value: getCurrentScrollPosition() });
                });
                $(window).on("resize", initializeProgressElement);
            } else {
                resizeProgressBar();
                $(document).on("scroll", resizeProgressBar);
                $(window).on("resize", resizeProgressBar);
            }
        }
        /*
        * The vertical scroll position is the same as the number of pixels that
        * are hidden from view above the scrollable area. Thus, a value > 0 is
        * how much the user has scrolled from the top
        */
        function getCurrentScrollPosition() {
            return $(window).scrollTop();
        }

        function initializeProgressElement() {
            let navbarHeight = $("#navbar").outerHeight(true);
            $("body").css({ "padding-top": navbarHeight });
            $("progress-container").css({ "padding-top": navbarHeight });
            progressBar.css({ top: navbarHeight });
            progressBar.attr({
                max: getDistanceToScroll(),
                value: getCurrentScrollPosition(),
            });
        }
        /*
        * The offset between the html document height and the browser viewport
        * height will be greater than zero if vertical scroll is possible.
        * This is the distance the user can scroll
        */
        function getDistanceToScroll() {
            return $(document).height() - $(window).height();
        }

        function resizeProgressBar() {
            progressBar.css({ width: getWidthPercentage() + "%" });
        }
        // The scroll ratio equals the percentage to resize the bar
        function getWidthPercentage() {
            return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
        }
    </script>

    </body>
</html>
